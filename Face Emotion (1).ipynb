{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os  \n",
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense, Dropout, Activation, Flatten  \n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D  \n",
    "from keras.losses import categorical_crossentropy  \n",
    "from keras.optimizers import Adam  \n",
    "from keras.regularizers import l2  \n",
    "from keras.utils import np_utils  \n",
    "# pd.set_option('display.max_rows', 500)  \n",
    "# pd.set_option('display.max_columns', 500)  \n",
    "# pd.set_option('display.width', 1000)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('fer2013.csv')  \n",
    "\n",
    "# print(df.info())  \n",
    "# print(df[\"Usage\"].value_counts())  \n",
    "\n",
    "# print(df.head())  \n",
    "X_train,train_y,X_test,test_y=[],[],[],[]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fer2013_show_instance(index):\n",
    "    \"\"\"Shows the image and the emotin labl of the index's instance\"\"\"\n",
    "    image = np.reshape(training_data.at[index, \"pixels\"].split(\" \"), (width, height)).astype(\"float\")\n",
    "    image -= np.mean(image)\n",
    "    image /= np.std(image)\n",
    "    print(emotion_labels[training_data.at[index, \"emotion\"]])\n",
    "    plt.imshow(image, cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Training       28709\n",
       "PrivateTest     3589\n",
       "PublicTest      3589\n",
       "Name: Usage, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Usage.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [i for _, i in df.groupby('Usage')]\n",
    "training_data = groups[2]\n",
    "validation_data = groups[1]\n",
    "testing_data= groups[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "width = 48\n",
    "height = 48\n",
    "emotion_labels = [\"Angry\",\"Disgust\",\"fear\",\"Happy\",\"Sad\",\"Surprise\",\"Neutral\"]\n",
    "Classes = np.array([\"Angry\",\"Disgust\",\"fear\",\"Happy\",\"Sad\",\"Surprise\",\"Neutral\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fer2013_show_instance(np.random.randint(90,len(training_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():  \n",
    "    val=row['pixels'].split(\" \")  \n",
    "    try:  \n",
    "        if 'Training' in row['Usage']:  \n",
    "           X_train.append(np.array(val,'float32'))  \n",
    "           train_y.append(row['emotion'])  \n",
    "        elif 'PublicTest' in row['Usage']:  \n",
    "           X_test.append(np.array(val,'float32'))  \n",
    "           test_y.append(row['emotion'])  \n",
    "    except:  \n",
    "        print(f\"error occured at index :{index} and row:{row}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 64  \n",
    "num_labels = 7  \n",
    "batch_size = 64  \n",
    "epochs = 30  \n",
    "width, height = 48, 48  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train,'float32')  \n",
    "train_y = np.array(train_y,'float32')  \n",
    "X_test = np.array(X_test,'float32')  \n",
    "test_y = np.array(test_y,'float32')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y=np_utils.to_categorical(train_y, num_classes=num_labels)  \n",
    "test_y=np_utils.to_categorical(test_y, num_classes=num_labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cannot produce  \n",
    "#normalizing data between oand 1  \n",
    "X_train -= np.mean(X_train, axis=0)  \n",
    "X_train /= np.std(X_train, axis=0)  \n",
    "\n",
    "X_test -= np.mean(X_test, axis=0)  \n",
    "X_test /= np.std(X_test, axis=0)  \n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)  \n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"shape:{X_train.shape}\")  \n",
    "##designing the cnn  \n",
    "#1st convolution layer  \n",
    "model = Sequential()  \n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1:])))  \n",
    "model.add(Conv2D(64,kernel_size= (3, 3), activation='relu'))  \n",
    "# model.add(BatchNormalization())  \n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))  \n",
    "model.add(Dropout(0.5))  \n",
    "\n",
    "#2nd convolution layer  \n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))  \n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))  \n",
    "# model.add(BatchNormalization())  \n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))  \n",
    "model.add(Dropout(0.5))  \n",
    "\n",
    "#3rd convolution layer  \n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))  \n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))  \n",
    "# model.add(BatchNormalization())  \n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))  \n",
    "\n",
    "model.add(Flatten())  \n",
    "\n",
    "#fully connected neural networks  \n",
    "model.add(Dense(1024, activation='relu'))  \n",
    "model.add(Dropout(0.2))  \n",
    "model.add(Dense(1024, activation='relu'))  \n",
    "model.add(Dropout(0.2))  \n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax'))  \n",
    "\n",
    "# model.summary()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compliling the model  \n",
    "model.compile(loss=categorical_crossentropy,  \n",
    "              optimizer=Adam(),  \n",
    "              metrics=['accuracy'])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "449/449 [==============================] - 361s 793ms/step - loss: 1.7942 - accuracy: 0.2533 - val_loss: 1.6165 - val_accuracy: 0.3430\n",
      "Epoch 2/30\n",
      "449/449 [==============================] - 352s 783ms/step - loss: 1.5817 - accuracy: 0.3741 - val_loss: 1.4567 - val_accuracy: 0.4283\n",
      "Epoch 3/30\n",
      "449/449 [==============================] - 351s 783ms/step - loss: 1.4535 - accuracy: 0.4376 - val_loss: 1.3460 - val_accuracy: 0.4776\n",
      "Epoch 4/30\n",
      "449/449 [==============================] - 352s 784ms/step - loss: 1.3693 - accuracy: 0.4713 - val_loss: 1.3117 - val_accuracy: 0.4999\n",
      "Epoch 5/30\n",
      "449/449 [==============================] - 350s 780ms/step - loss: 1.3199 - accuracy: 0.4885 - val_loss: 1.2833 - val_accuracy: 0.4965\n",
      "Epoch 6/30\n",
      "449/449 [==============================] - 352s 785ms/step - loss: 1.2840 - accuracy: 0.5087 - val_loss: 1.2414 - val_accuracy: 0.5199\n",
      "Epoch 7/30\n",
      "449/449 [==============================] - 352s 783ms/step - loss: 1.2684 - accuracy: 0.5094 - val_loss: 1.2262 - val_accuracy: 0.5300\n",
      "Epoch 8/30\n",
      "449/449 [==============================] - 360s 802ms/step - loss: 1.2359 - accuracy: 0.5264 - val_loss: 1.2009 - val_accuracy: 0.5400\n",
      "Epoch 9/30\n",
      "449/449 [==============================] - 356s 793ms/step - loss: 1.2219 - accuracy: 0.5319 - val_loss: 1.2220 - val_accuracy: 0.5322\n",
      "Epoch 10/30\n",
      "449/449 [==============================] - 352s 784ms/step - loss: 1.1890 - accuracy: 0.5444 - val_loss: 1.1850 - val_accuracy: 0.5536\n",
      "Epoch 11/30\n",
      "449/449 [==============================] - 352s 784ms/step - loss: 1.1779 - accuracy: 0.5462 - val_loss: 1.1891 - val_accuracy: 0.5542\n",
      "Epoch 12/30\n",
      "449/449 [==============================] - 353s 787ms/step - loss: 1.1496 - accuracy: 0.5592 - val_loss: 1.1708 - val_accuracy: 0.5458\n",
      "Epoch 13/30\n",
      "449/449 [==============================] - 351s 781ms/step - loss: 1.1421 - accuracy: 0.5630 - val_loss: 1.2045 - val_accuracy: 0.5386\n",
      "Epoch 14/30\n",
      "449/449 [==============================] - 352s 784ms/step - loss: 1.1259 - accuracy: 0.5690 - val_loss: 1.1796 - val_accuracy: 0.5589\n",
      "Epoch 15/30\n",
      "449/449 [==============================] - 351s 781ms/step - loss: 1.1036 - accuracy: 0.5770 - val_loss: 1.1561 - val_accuracy: 0.5550\n",
      "Epoch 16/30\n",
      "449/449 [==============================] - 353s 786ms/step - loss: 1.0867 - accuracy: 0.5831 - val_loss: 1.1447 - val_accuracy: 0.5637\n",
      "Epoch 17/30\n",
      "449/449 [==============================] - 351s 782ms/step - loss: 1.0812 - accuracy: 0.5848 - val_loss: 1.1598 - val_accuracy: 0.5575\n",
      "Epoch 18/30\n",
      "449/449 [==============================] - 352s 784ms/step - loss: 1.0679 - accuracy: 0.5958 - val_loss: 1.1560 - val_accuracy: 0.5617\n",
      "Epoch 19/30\n",
      "449/449 [==============================] - 352s 784ms/step - loss: 1.0522 - accuracy: 0.5941 - val_loss: 1.1662 - val_accuracy: 0.5659\n",
      "Epoch 20/30\n",
      "449/449 [==============================] - 350s 780ms/step - loss: 1.0367 - accuracy: 0.6007 - val_loss: 1.1512 - val_accuracy: 0.5598\n",
      "Epoch 21/30\n",
      "449/449 [==============================] - 351s 783ms/step - loss: 1.0223 - accuracy: 0.6044 - val_loss: 1.1581 - val_accuracy: 0.5704\n",
      "Epoch 22/30\n",
      "449/449 [==============================] - 351s 781ms/step - loss: 1.0104 - accuracy: 0.6157 - val_loss: 1.1391 - val_accuracy: 0.5692\n",
      "Epoch 23/30\n",
      "449/449 [==============================] - 354s 789ms/step - loss: 0.9951 - accuracy: 0.6185 - val_loss: 1.1459 - val_accuracy: 0.5687\n",
      "Epoch 24/30\n",
      "449/449 [==============================] - 354s 789ms/step - loss: 0.9982 - accuracy: 0.6211 - val_loss: 1.1668 - val_accuracy: 0.5631\n",
      "Epoch 25/30\n",
      "449/449 [==============================] - 349s 778ms/step - loss: 0.9924 - accuracy: 0.6186 - val_loss: 1.1572 - val_accuracy: 0.5717\n",
      "Epoch 26/30\n",
      "449/449 [==============================] - 351s 781ms/step - loss: 0.9636 - accuracy: 0.6336 - val_loss: 1.1423 - val_accuracy: 0.5726\n",
      "Epoch 27/30\n",
      "449/449 [==============================] - 349s 778ms/step - loss: 0.9534 - accuracy: 0.6402 - val_loss: 1.1383 - val_accuracy: 0.5882\n",
      "Epoch 28/30\n",
      "449/449 [==============================] - 350s 779ms/step - loss: 0.9469 - accuracy: 0.6383 - val_loss: 1.1507 - val_accuracy: 0.5790\n",
      "Epoch 29/30\n",
      "449/449 [==============================] - 351s 781ms/step - loss: 0.9370 - accuracy: 0.6411 - val_loss: 1.1540 - val_accuracy: 0.5784\n",
      "Epoch 30/30\n",
      "449/449 [==============================] - 348s 774ms/step - loss: 0.9250 - accuracy: 0.6473 - val_loss: 1.1613 - val_accuracy: 0.5706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1df48498460>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the model  \n",
    "model.fit(X_train, train_y,  \n",
    "          batch_size=batch_size,  \n",
    "          epochs=epochs,  \n",
    "          verbose=1,  \n",
    "          validation_data=(X_test, test_y),  \n",
    "          shuffle=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the  model to  use it later on  \n",
    "fer_json = model.to_json()  \n",
    "with open(\"fer.json\", \"w\") as json_file:  \n",
    "    json_file.write(fer_json)  \n",
    "model.save_weights(\"fer.h5\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at D:/DataFlair/FER_Model.h5 \n"
     ]
    }
   ],
   "source": [
    "model_name = 'FER_Model.h5'\n",
    "save_dir = 'D:/DataFlair/'\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array\n",
    "import imutils\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for loading data and images\n",
    "detection_model_path = 'haarcascade_frontalface_default.xml'\n",
    "emotion_model_path = 'FER_Model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters for bounding boxes shape\n",
    "# loading models\n",
    "face_detection = cv2.CascadeClassifier(detection_model_path)\n",
    "emotion_classifier = load_model(emotion_model_path, compile=False)\n",
    "EMOTIONS = [\"angry\" ,\"disgust\",\"scared\", \"happy\", \"sad\", \"surprised\",\n",
    " \"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feelings_faces = []\n",
    "#for index, emotion in enumerate(EMOTIONS):\n",
    "   # feelings_faces.append(cv2.imread('emojis/' + emotion + '.png', -1))\n",
    "import cv2\n",
    "# starting video streaming\n",
    "cv2.namedWindow('your_face')\n",
    "camera = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    frame = camera.read()[1]\n",
    "    #reading the frame\n",
    "    frame = imutils.resize(frame,width=300)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_detection.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=5,minSize=(30,30),flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    \n",
    "    canvas = np.zeros((250, 300, 3), dtype=\"uint8\")\n",
    "    frameClone = frame.copy()\n",
    "    if len(faces) > 0:\n",
    "        faces = sorted(faces, reverse=True,\n",
    "        key=lambda x: (x[2] - x[0]) * (x[3] - x[1]))[0]\n",
    "        (fX, fY, fW, fH) = faces\n",
    "                    # Extract the ROI of the face from the grayscale image, resize it to a fixed 28x28 pixels, and then prepare\n",
    "            # the ROI for classification via the CNN\n",
    "        roi = gray[fY:fY + fH, fX:fX + fW]\n",
    "        roi = cv2.resize(roi, (48, 48))\n",
    "        roi = roi.astype(\"float\") / 255.0\n",
    "        roi = img_to_array(roi)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "        \n",
    "        \n",
    "        preds = emotion_classifier.predict(roi)[0]\n",
    "        emotion_probability = np.max(preds)\n",
    "        label = EMOTIONS[preds.argmax()]\n",
    "    else: continue\n",
    "\n",
    " \n",
    "    for (i, (emotion, prob)) in enumerate(zip(EMOTIONS, preds)):\n",
    "                # construct the label text\n",
    "                text = \"{}: {:.2f}%\".format(emotion, prob * 100)\n",
    "\n",
    "                # draw the label + probability bar on the canvas\n",
    "               # emoji_face = feelings_faces[np.argmax(preds)]\n",
    "\n",
    "                \n",
    "                w = int(prob * 300)\n",
    "                cv2.rectangle(canvas, (7, (i * 35) + 5),\n",
    "                (w, (i * 35) + 35), (0, 0, 255), -1)    \n",
    "                cv2.putText(canvas, text, (10, (i * 35) + 23),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.45,\n",
    "                (255, 255, 255), 2)\n",
    "                cv2.putText(frameClone, label, (fX, fY - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "                cv2.rectangle(frameClone, (fX, fY), (fX + fW, fY + fH),\n",
    "                              (0, 0, 255), 2)\n",
    "#    for c in range(0, 3):\n",
    "#        frame[200:320, 10:130, c] = emoji_face[:, :, c] * \\\n",
    "#        (emoji_face[:, :, 3] / 255.0) + frame[200:320,\n",
    "#        10:130, c] * (1.0 - emoji_face[:, :, 3] / 255.0)\n",
    "\n",
    "    cv2.imshow('your_face', frameClone)\n",
    "    cv2.imshow(\"Probabilities\", canvas)                                                                                                                                                                                                                                    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
